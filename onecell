{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYcVF+pZdhKZzFtLln7cJ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayuriMore10/practical/blob/main/onecell\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "sEVeYRgzOOgT",
        "outputId": "815b2fd5-bf3d-4c12-8342-49743f137329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['there', 'was', 'girl', 'named', 'mari', 'she', 'was', 'living', 'alone', 'in', 'the', 'small', 'hut', 'her', 'parents', 'died', 'when', 'she', 'was', 'she', 'used', 'to', 'go', 'temple', 'daily', 'now', 'she', 'is', '20', 'working', 'at', 'mnc', 'she', 'has', 'married', 'to', 'rich', 'man']\n",
            "{'small', 'married', 'daily', 'at', 'rich', '20', 'girl', 'has', 'was', 'the', 'parents', 'is', 'when', 'her', 'mnc', 'she', 'temple', 'named', 'living', 'hut', 'go', 'mari', 'working', 'now', 'man', 'died', 'used', 'to', 'in', 'alone', 'there'}\n",
            "{'small': 0, 'married': 1, 'daily': 2, 'at': 3, 'rich': 4, '20': 5, 'girl': 6, 'has': 7, 'was': 8, 'the': 9, 'parents': 10, 'is': 11, 'when': 12, 'her': 13, 'mnc': 14, 'she': 15, 'temple': 16, 'named': 17, 'living': 18, 'hut': 19, 'go': 20, 'mari': 21, 'working': 22, 'now': 23, 'man': 24, 'died': 25, 'used': 26, 'to': 27, 'in': 28, 'alone': 29, 'there': 30}\n",
            "===========================\n",
            "{0: 'small', 1: 'married', 2: 'daily', 3: 'at', 4: 'rich', 5: '20', 6: 'girl', 7: 'has', 8: 'was', 9: 'the', 10: 'parents', 11: 'is', 12: 'when', 13: 'her', 14: 'mnc', 15: 'she', 16: 'temple', 17: 'named', 18: 'living', 19: 'hut', 20: 'go', 21: 'mari', 22: 'working', 23: 'now', 24: 'man', 25: 'died', 26: 'used', 27: 'to', 28: 'in', 29: 'alone', 30: 'there'}\n",
            "[(['there', 'was', 'named', 'mari'], 'girl'), (['was', 'girl', 'mari', 'she'], 'named'), (['girl', 'named', 'she', 'was'], 'mari'), (['named', 'mari', 'was', 'living'], 'she'), (['mari', 'she', 'living', 'alone'], 'was')]\n",
            "[[0.17714012 0.16823284 0.62825063 0.36091977 0.72100288 0.08343205\n",
            "  0.19540537 0.76857149 0.02589552 0.01312584]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'has'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import\tmatplotlib.pyplot\tas\tplt\n",
        "import\tseaborn\tas\tsns\n",
        "import\tmatplotlib\tas\tmpl\n",
        "import\tmatplotlib.pylab\tas\tpylab\n",
        "import\tnumpy\tas\tnp\n",
        "\n",
        "import re\n",
        "sentences=\"\"\"There was girl named mari.She was living alone in the small hut.Her parents died when she was 8.She used to go temple daily.Now she is 20.working at MNC.She has married to rich man\"\"\"\n",
        " #\tremove\tspecial\tcharacters\n",
        "sentences=re.sub('[^A-Za-z0-9]+',\t'\t',\tsentences)\n",
        " #\tremove\t1\tletter\twords\n",
        "sentences\t=\tre.sub(r'(?:^|\t)\\w(?:$|\t)',\t'\t',\tsentences).strip()\n",
        " #\tlower\tall\tcharacters\n",
        "sentences\t=\tsentences.lower()\n",
        "sentences\n",
        "words\t=\tsentences.split()\n",
        "print(words)\n",
        "vocab\t=\tset(words)\n",
        "print(vocab)\n",
        "vocab_size\t=\tlen(vocab)\n",
        "embed_dim\t=\t10\n",
        "context_size\t=\t2\n",
        "word_to_ix\t=\t{word:\ti\tfor\ti,\tword\tin\tenumerate(vocab)}\n",
        "ix_to_word\t=\t{i:\tword\tfor\ti,\tword\tin\tenumerate(vocab)}\n",
        "print(word_to_ix)\n",
        "print(\"===========================\")\n",
        "print(ix_to_word)\n",
        "data\t=\t[]\n",
        "for\ti\tin\trange(2,\tlen(words)\t-\t2):\n",
        "  context\t=\t[words[i\t-\t2],\twords[i\t-\t1],\twords[i\t+\t1],\twords[i\t+\t2]]\n",
        "  target\t=\twords[i]\n",
        "  data.append((context,\ttarget))\n",
        "print(data[:5])\n",
        "embeddings\t=\t\tnp.random.random_sample((vocab_size,\tembed_dim))\n",
        "print(embeddings[:1])\n",
        "def\tlinear(m,\ttheta):\n",
        " w\t=\ttheta\n",
        " return\tm.dot(w)\n",
        "def\tlog_softmax(x):\n",
        " e_x\t=\tnp.exp(x\t-\tnp.max(x))\n",
        " return\tnp.log(e_x\t/\te_x.sum())\n",
        "def\tNLLLoss(logs,\ttargets):\n",
        " out\t=\tlogs[range(len(targets)),\ttargets]\n",
        " return\t-out.sum()/len(out)\n",
        "def\tlog_softmax_crossentropy_with_logits(logits,target):\n",
        " out\t=\tnp.zeros_like(logits)\n",
        " out[np.arange(len(logits)),target]\t=\t1\n",
        " softmax\t=\tnp.exp(logits)\t/\tnp.exp(logits).sum(axis=-1,keepdims=True)\n",
        " return\t(-\tout\t+\tsoftmax)\t/\tlogits.shape[0]\n",
        "def\tforward(context_idxs,\ttheta):\n",
        " m\t=\tembeddings[context_idxs].reshape(1,\t-1)\n",
        " n\t=\tlinear(m,\ttheta)\n",
        " o\t=\tlog_softmax(n)\n",
        " return\tm,\tn,\to\n",
        "def\tbackward(preds,\ttheta,\ttarget_idxs):\n",
        " m,\tn,\to\t=\tpreds\n",
        " dlog\t=\tlog_softmax_crossentropy_with_logits(n,\ttarget_idxs)\n",
        " dw\t=\tm.T.dot(dlog)\n",
        " return\tdw\n",
        "def\toptimize(theta,\tgrad,\tlr=0.03):\n",
        " theta\t-=\tgrad\t*\tlr\n",
        " return\ttheta\n",
        "theta\t=\tnp.random.uniform(-1,\t1,\t(2\t*\tcontext_size\t*\tembed_dim,\tvocab_size))\n",
        "epoch_losses\t=\t{}\n",
        "for\tepoch\tin\trange(80):\n",
        "  losses\t=\t\t[]\n",
        "  for\tcontext,\ttarget\tin\tdata:\n",
        "    context_idxs\t=\tnp.array([word_to_ix[w]\tfor\tw\tin\tcontext])\n",
        "    preds\t=\tforward(context_idxs,\ttheta)\n",
        "    target_idxs\t=\tnp.array([word_to_ix[target]])\n",
        "    loss\t=\tNLLLoss(preds[-1],\ttarget_idxs)\n",
        "    losses.append(loss)\n",
        "    grad\t=\tbackward(preds,\ttheta,\ttarget_idxs)\n",
        "    theta\t=\toptimize(theta,\tgrad,\tlr=0.03)\n",
        "  epoch_losses[epoch]\t=\tlosses\n",
        "def\tpredict(words):\n",
        " context_idxs\t=\tnp.array([word_to_ix[w]\tfor\tw\tin\twords])\n",
        " preds\t=\tforward(context_idxs,\ttheta)\n",
        " word\t=\tix_to_word[np.argmax(preds[-1])]\n",
        " return\tword\n",
        "predict(['daily',\t'to',\t'used',\t'temple'])\n",
        "def\taccuracy():\n",
        " wrong\t=\t0\n",
        " for\tcontext,\ttarget\tin\tdata:\n",
        "  if(predict(context)\t!=\ttarget):\n",
        "    wrong\t+=\t1\n",
        " return\t(1\t-\t(wrong\t/\tlen(data)))\n",
        "accuracy()\n",
        "predict(['she','has','married','man'])\n",
        "\n"
      ]
    }
  ]
}